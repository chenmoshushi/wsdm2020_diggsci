{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自行更换模型配置路径\n",
    "\n",
    "6类模型:\n",
    "1. biobert v1.1 * 3\n",
    "2. biobert v1.0 pubmed_pmc * 2\n",
    "3. biobert_dish * 1\n",
    "4. biobert_v1.0 pubmd\n",
    "5. biobert_v1.0 pmc\n",
    "6. scibert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "from keras.callbacks import *\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, f1_score\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# chinese_wwm_ext_L-12_H-768_A-12\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "session = tf.Session(config=config)\n",
    "# 设置session\n",
    "KTF.set_session(session)\n",
    "\n",
    "config_path = './biobert_v1.1_pubmed/bert_config.json'\n",
    "checkpoint_path = './biobert_v1.1_pubmed/bert_model.ckpt'\n",
    "dict_path = './biobert_v1.1_pubmed/vocab.txt'\n",
    "# MAX_LEN = 224\n",
    "MAX_LEN = 300\n",
    "n_class = 2\n",
    "\n",
    "token_dict = {}\n",
    "with open(dict_path, 'r', encoding='utf-8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "tokenizer = Tokenizer(token_dict, cased=True)\n",
    "\n",
    "class data_generator:\n",
    "    def __init__(self, data, batch_size=16):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data[0]) // self.batch_size\n",
    "        if len(self.data[0]) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            X1, X2, y = self.data\n",
    "            idxs = list(range(len(self.data[0])))\n",
    "            # np.random.shuffle(idxs)\n",
    "            T, T_, Y = [], [], []\n",
    "            for c, i in enumerate(idxs):\n",
    "                achievements = X1[i]\n",
    "                requirements = X2[i]\n",
    "                t, t_ = tokenizer.encode(first=achievements, second=requirements, max_len=MAX_LEN)\n",
    "                T.append(t)\n",
    "                T_.append(t_)\n",
    "                Y.append(y[i])\n",
    "                if len(T) == self.batch_size or i == idxs[-1]:\n",
    "                    T = np.array(T)\n",
    "                    T_ = np.array(T_)\n",
    "                    Y = np.array(Y)\n",
    "#                     print(Y)\n",
    "                    yield [T, T_], Y\n",
    "                    T, T_, Y = [], [], []\n",
    "\n",
    "def get_model():\n",
    "    bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    "\n",
    "    T1 = Input(shape=(None,))\n",
    "    T2 = Input(shape=(None,))\n",
    "    \n",
    "    T = bert_model([T1, T2])\n",
    "\n",
    "    T = Lambda(lambda x: x[:, 0])(T)\n",
    "    \n",
    "    output = Dense(n_class, activation='softmax')(T)\n",
    "\n",
    "    model = Model([T1, T2], output)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=Adam(1e-5),  # 用足够小的学习率\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "train = pd.read_csv('./new_data/train_data3_bm2540_tfidf20_6.csv')\n",
    "# test = pd.read_csv('./new_data/stage2_test_data3_bm2580_tfidf20_8.csv')\n",
    "# test = pd.read_csv('./new_data/stage2_test_data3_bm2590_tfidf20_10.csv')\n",
    "# test = pd.read_csv('./new_data/stage2_test_data3_bm25100_tfidf20_11.csv')\n",
    "\n",
    "# test = test[:256]\n",
    "\n",
    "train_achievements = train['text'].values\n",
    "train_requirements = train['text_b'].values\n",
    "\n",
    "labels = train['label'].astype(int).values\n",
    "labels_cat = to_categorical(labels,2)\n",
    "labels_cat = labels_cat.astype(np.int32)\n",
    "\n",
    "test_achievements = test['text'].values\n",
    "test_requirements = test['text_b'].values\n",
    "test_cat = to_categorical([0 for x in test_achievements],n_class)\n",
    "\n",
    "n_flods = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_flods, shuffle=True, random_state=256)\n",
    "\n",
    "oof_train = np.zeros((len(train), labels_cat.shape[1]), dtype=np.float32)\n",
    "oof_test = np.zeros((len(test), labels_cat.shape[1]), dtype=np.float32)\n",
    "\n",
    "oof_test_list = []\n",
    "\n",
    "# submit_D = data_generator([test_achievements, test_requirements, test_cate, to_categorical([0 for x in test_achievements],n_class)],batch_size=32)\n",
    "submit_D = data_generator([test_achievements, test_requirements, test_cat],batch_size=512)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(train_achievements, labels)):\n",
    "#     if fold >=1 and fold != 3:\n",
    "    if fold >= 0:\n",
    "        K.clear_session()   \n",
    "        # logger.info('================     fold {}        ==============='.format(fold))\n",
    "        x1 = train_achievements[train_index]\n",
    "        x2 = train_requirements[train_index]\n",
    "        y = labels_cat[train_index]\n",
    "\n",
    "        val_x1 = train_achievements[valid_index]\n",
    "        val_x2 = train_requirements[valid_index]\n",
    "        val_y = labels[valid_index]\n",
    "        val_cat = labels_cat[valid_index]\n",
    "\n",
    "        train_D = data_generator([x1, x2, y],batch_size=8)\n",
    "        valid_D = data_generator([val_x1, val_x2, val_cat],batch_size=512)\n",
    "\n",
    "        checkpointer = ModelCheckpoint(filepath=\"./checkpoint_%d.hdf5\" % (fold), monitor='val_acc', verbose=True,\n",
    "                                       save_best_only=True, mode='auto')\n",
    "        early = EarlyStopping(monitor='val_acc', patience=2, verbose=0, mode='max')\n",
    "\n",
    "        model = get_model()\n",
    "        #model.load_weights(\"./checkpoint_0_bert5_single_fold.hdf5\")\n",
    "        if fold==0:print(model.summary())\n",
    "\n",
    "        if fold >= 0:   \n",
    "            model.fit_generator(train_D.__iter__(),\n",
    "                                steps_per_epoch=len(train_D),\n",
    "#                                 steps_per_epoch=1000,\n",
    "                                epochs=1,\n",
    "                                validation_data=valid_D.__iter__(),\n",
    "#                                 validation_steps=len(valid_D),\n",
    "                                validation_steps=1,\n",
    "                                callbacks=[checkpointer, early],\n",
    "    #                             callbacks=[checkpointer],\n",
    "                                verbose=True\n",
    "                               )\n",
    "        model.load_weights(\"./checkpoint_%d.hdf5\" % (fold))\n",
    "#         test_y = model.predict_generator(valid_D.__iter__(), steps=len(valid_D), verbose=1)\n",
    "#         oof_train[valid_index] = test_y\n",
    "\n",
    "        tmp_tmp = model.predict_generator(submit_D.__iter__(), steps=len(submit_D), verbose=1)\n",
    "        pd.DataFrame(tmp_tmp).to_csv('tmp/biobert6_fold'+str(fold)+'_dataset3.csv', index=False)\n",
    "#         oof_test_list.append(tmp_tmp)\n",
    "        oof_test += tmp_tmp / n_flods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
